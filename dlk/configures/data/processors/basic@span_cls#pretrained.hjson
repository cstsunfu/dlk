{
    "_name": "basic@span_cls#pretrained",
    "config": {
        "feed_order": [
            "load",
            "seq_lab_loader",
            "tokenizer",
            "label_gather",
            "span_cls_relabel",
            "save"
        ],
        "tokenizer_config_path": "*@*", # the tokenizer config path (the tokenizer.json path)
        "data_dir": "*@*", # save load data base dir
    },
    "_link": {
        "subprocessor@tokenizer.config.train.output_map.word_ids": "subprocessor@span_cls_relabel.config.train.input_map.word_ids",
        "config.tokenizer_config_path": [
            "subprocessor@tokenizer.config.train.config_path"
        ],
        "config.data_dir": [
            "subprocessor@load.config.base_dir",
            "subprocessor@save.config.base_dir"
        ],
    },
    "subprocessor@load": {
        "_base": "load",
    },
    "subprocessor@save": {
        "_base": "save",
        "config": {
            "base_dir": "",
            "train": {
                "processed": "processed_data.pkl", # all data
                "meta": {
                    "meta.pkl": ['label_vocab',
                        "tokenizer"
                    ] #only for next time use
                }
            },
            "predict": {
                "processed": "processed_data.pkl",
            }
        }
    },
    "subprocessor@seq_lab_loader": { # the span_cls dataloader is the same as seq_lab
        "_base": "basic_data_loader@seq_lab",
    },
    "subprocessor@tokenizer": {
        "_base": "fast_tokenizer",
        "config": {
            "train": {
                "config_path": "*@*",
                "deliver": "tokenizer",
                "data_type": "single", # single or pair, if not provide, will calc by len(process_data)
                "process_data": {
                    "is_pretokenized": false
                },
                "input_map": {
                    "sentence": "sentence", #for sigle input, tokenizer the "sentence"
                    "sentence_a": "sentence_a", #for pair inputs, tokenize the "sentence_a" && "sentence_b"
                    "sentence_b": "sentence_b", #for pair inputs
                },
            },
            "predict": "train",
            "online": "train"
        }
    },
    "subprocessor@label_gather": {
        "_base": "token_gather",
        "config": {
            "train": { # only train stage using
                "data_set": { # for different stage, this processor will process different part of data
                    "train": [
                        "train",
                        "valid"
                    ]
                },
                "deliver": "label_vocab", # output Vocabulary object (the Vocabulary of labels) name.
                "gather_columns": [
                    {
                        "column": "entities_info",
                        "trace": 'labels'
                    }
                ],
                "unk": "[UNK]",
                "pad": "",
            }
        }
    },
    "subprocessor@span_cls_relabel": {
        "_base": "span_cls_relabel",
        "config": {
            "train": {
                "drop": "none", #'longer'/'shorter'/'none', if entities is overlap, will remove by rule
            },
        }
    },
}
