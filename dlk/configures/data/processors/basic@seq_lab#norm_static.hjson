{
    // input should be {"train": train, "valid": valid, ...}, train/valid/test/predict/online etc, should be dataframe and must have a column named "sentence"
    "_name": "basic@seq_lab#static",
    "config": {
        /*"feed_order": ["load", "seq_lab_loader", "token_norm", "tokenizer", "token_gather", "save"]*/
        "feed_order": ["load", "seq_lab_loader", "token_norm", "tokenizer", "seq_lab_relabel", 'label_gather', "label_to_id", "token_embedding", "save"]
        "tokenizer_config_path": "*@*", // the tokenizer config path (the tokenizer.json path)
        "data_dir": "*@*",  // save load data base dir
        "embedding_size": "*@*",  //the embedding size
        "embedding_file": "*@*",
    },
    "_link": {
        "subprocessor@tokenizer.config.train.output_map.word_ids": "subprocessor@seq_lab_relabel.config.train.input_map.word_ids",
        "subprocessor@seq_lab_relabel.config.train.output_map.labels": "subprocessor@label_gather.config.train.gather_columns.0",
        "subprocessor@token_norm.config.train.data_pair.sentence": "subprocessor@tokenizer.config.train.input_map.sentence",
        "config.tokenizer_config_path": ["subprocessor@tokenizer.config.train.config_path", 'subprocessor@token_embedding.config.train.tokenizer', "subprocessor@token_norm.config.train.tokenizer"],
        "config.data_dir": ["subprocessor@load.config.base_dir", "subprocessor@save.config.base_dir"],
        "config.embedding_size": "subprocessor@token_embedding.config.train.embedding_size",
        "config.embedding_file": "subprocessor@token_embedding.config.train.embedding_file",
    },
    "subprocessor@load": {
        "_base": "load",
    },
    "subprocessor@save": {
        "_base": "save",
        "config":{
            "base_dir": "."
            "train":{
                "processed": "processed_data.pkl", // all data
                "meta": {
                    "meta.pkl": ['label_vocab', "tokenizer", 'token_embedding'], //only for next time use
                    /*"meta.pkl": ['token_vocab'] //only for next time use*/
                }
            },
            "predict": {
                "processed": "processed_data.pkl",
            }
        }
    },
    "subprocessor@seq_lab_loader":{
        "_base": "seq_lab_loader",
    },
    "subprocessor@token_norm":{
        "_base": "token_norm",
        "config": {
            "train": {
                "data_pair": {
                    "sentence": "norm_sentence",
                },
                "tokenizer": "*@*", // "the path to vocab(if the token in vocab skip norm it), the file is setted to one token per line",
            },
            "predict": "train",
            "online": "train"
        }
    },
    "subprocessor@tokenizer":{
        "_base": "fast_tokenizer",
        "config": {
            "train": {
                "config_path": "*@*",
                "deliver": "tokenizer",
                "data_type": "single", // single or pair, if not provide, will calc by len(process_data)
                "process_data": { "is_pretokenized": false},
                "input_map": {
                    "sentence": "sentence", //for sigle input, tokenizer the "sentence"
                },
            },
            "predict": "train",
            "online": "train",
        }
    },
    "subprocessor@seq_lab_relabel":{
        "_base": "seq_lab_relabel",
        "config": {
            "train":{ //train、predict、online stage config,  using '&' split all stages
                "start_label": "S",
                "end_label": "E",
                "output_map": {
                    "labels": "labels",
                },
            }, //3
        }
    },
    "subprocessor@token_gather":{
        "_base": "token_gather",
        "config": {
            "train": { // only train stage using
                "data_set": {      // for different stage, this processor will process different part of data
                    "train": ["train", "valid", 'test']
                },
                "gather_columns": ["tokens"], //List of columns. Every cell must be sigle token or list of tokens or set of tokens
                "deliver": "token_vocab", // output Vocabulary object (the Vocabulary of labels) name.
            }
        }
    },
    "subprocessor@label_gather":{
        "_base": "token_gather",
        "config": {
            "train": { // only train stage using
                "data_set": {      // for different stage, this processor will process different part of data
                    "train": ["train", "valid"]
                },
                "gather_columns": ["labels"], //List of columns. Every cell must be sigle token or list of tokens or set of tokens
                "unk": "",
                "pad": "",
                "deliver": "label_vocab", // output Vocabulary object (the Vocabulary of labels) name.
            }
        }
    },
    "subprocessor@label_to_id":{
        "_base": "token2id",
        "config": {
            "train":{ //train、predict、online stage config,  using '&' split all stages
                "data_pair": {
                    "labels": "label_ids"
                },
                "vocab": "label_vocab", // usually provided by the "token_gather" module
            }, //3
            "predict": "train",
            "online": "train",
        }
    },

    "subprocessor@token_embedding": {
        "_base": "token_embedding",
        "config":{
            "train": { // only train stage using
                "embedding_file": "*@*",
                "tokenizer": "*@*", //List of columns. Every cell must be sigle token or list of tokens or set of tokens
                "deliver": "token_embedding", // output Vocabulary object (the Vocabulary of labels) name.
                "embedding_size": "*@*",
            }
        }
    },
}
