{
    "_name": "fast_tokenizer",
    "config": {
        "train": {
            "data_set": {                   # for different stage, this processor will process different part of data
                "train": ["train", "valid", 'test'],
                "predict": ["predict"],
                "online": ["online"]
            },
            "config_path": "*@*",
            "truncation": {  # if this is set to null or empty, will not do trunc
                "direction": "right", # default `right`, if set `left`, will reserve the rightest chars.
                "stride": 0, # if the sequence is very long, will split to multiple span, stride is the window slide
                "max_length": 512,
                "strategy": "longest_first", # Can be one of longest_first, only_first or only_second.
            },
            "normalizer": "default", # ["nfd", "lowercase", "strip_accents", "some_processor_need_config": {config}], # if don't set this, will use the default normalizer from config
                "pre_tokenizer": "default",# [{"whitespace": {}}], # if don't set this, will use the default normalizer from config
                "post_processor": "default", # "bert", # if don't set this, will use the default normalizer from config, WARNING: not support disable  the default setting( so the default tokenizer.post_tokenizer should be null and only setting in this configure)
                "output_map": { # this is the default value, you can provide other name
                    "tokens": "tokens",
                    "ids": "input_ids",
                    "attention_mask": "attention_mask",
                    "type_ids": "type_ids",
                    "special_tokens_mask": "special_tokens_mask",
                    "offsets": "offsets",
                    "word_ids": "word_ids",
                    "sequence_ids": "sequence_ids",
                }, # the tokenizer output(the key) map to the value
            "input_map": {
                "sentence": "sentence", # for sigle input, tokenizer the "sentence"
                    "sentence_a": "sentence_a", # for pair inputs, tokenize the "sentence_a" && "sentence_b"
                    "sentence_b": "sentence_b", # for pair inputs
                    "pretokenized_words": "pretokenized_words", # pretokenized word related to sentence
                    "pretokenized_words_a": "pretokenized_words_a", # pretokenized word b related to sentence_a
                    "pretokenized_words_b": "pretokenized_words_b", # pretokenized word b related to sentence_b
                    "pretokenized_word_offsets": "pretokenized_word_offsets", # pretokenized word offsets for fix offset
                    "pretokenized_word_offsets_a": "pretokenized_word_offsets_a", # pretokenized word offsets for fix offset
                    "pretokenized_word_offsets_b": "pretokenized_word_offsets_b", # pretokenized word offsets for fix offset
            },
            "deliver": "tokenizer",
            "process_data": { "is_pretokenized": false, "add_special_tokens": true},
            "expand_examples": true, # if the sequence is very long, will split to multiple span, whether expand the examples
            "data_type": "single", # single or pair, if not provide, will calc by len(process_data)
            "fix_offset": false, # whether fix the offset for pretokenizerd word
        },
        "predict": ["train", {"deliver": null}],
        "online": ["train", {"deliver": null}],
    }
}
