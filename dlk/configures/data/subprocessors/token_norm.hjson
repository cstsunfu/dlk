{
    "_name": "token_norm",
    "config": {
        "train":{
            "data_set": {                   // for different stage, this processor will process different part of data
                "train": ['train', 'valid', 'test', 'predict'],
                "predict": ['predict'],
                "online": ['online']
            },
            "zero_digits_replaced": true,
            "lowercase": true,
            "extend_vocab": null, //when lowercase is true, this upper_case_vocab will collection all tokens the token is not in vocab but it's lowercase is in vocab. this is only for token gather process
            "tokenizer": "whitespace_split",  //the path to vocab(if the token in vocab skip norm it), the file is setted to one token per line
            "data_pair": {
                "sentence": "norm_sentence"
            },
        },
        "predict": "train",
        "online": "train",
    }
}
