{
    "_base": "wordpiece_tokenizer",
    "config": {
        "train": { // you can add some whitespace surround the '&' 
            "data_set": {                   // for different stage, this processor will process different part of data
                "train": ["train", "dev"],
                "predict": ["predict"],
                "online": ["online"]
            },
            "config_path": "*@*",
            "normalizer": ["nfd", "lowercase", "strip_accents", "some_processor_need_config": {config}], // if don't set this, will use the default normalizer from config
            "pre_tokenizer": ["whitespace": {}], // if don't set this, will use the default normalizer from config
            "post_processor": "bert", // if don't set this, will use the default normalizer from config, WARNING: not support disable  the default setting( so the default tokenizer.post_tokenizer should be null and only setting in this configure)
            "filed_map": { // this is the default value, you can provide other name
                "tokens": "tokens",
                "ids": "ids",
                "attention_mask": "attention_mask",
                "type_ids": "type_ids",
                "special_tokens_mask": "special_tokens_mask",
                "offsets": "offsets",
            }, // the tokenizer output(the key) map to the value
            "data_type": "single", // single or pair, if not provide, will calc by len(process_data)
            "process_data": [
                ["sentence", { "is_pretokenized": false}], 
            ],
            /*"data_type": "pair", // single or pair*/
            /*"process_data": [*/
                /*['sentence_a', { "is_pretokenized": false}], */ 
                /*['sentence_b', {}], the config of the second data must as same as the first*/ 
            /*],*/
        },
        "predict": "train",
        "online": "train"
    }
}
