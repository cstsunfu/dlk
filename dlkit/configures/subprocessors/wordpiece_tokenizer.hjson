{
    "_name": "wordpiece_tokenizer",
    "config": {
        "train": { // you can add some whitespace surround the '&' 
            "data_set": {                   // for different stage, this processor will process different part of data
                "train": ["train", "dev"],
                "predict": ["predict"],
                "online": ["online"]
            },
            "config_path": "*@*",
            /*"normalizer": ["nfd", "lowercase", "strip_accents"], // if don't set this, will use the default normalizer from config*/
            "normalizer": "default", // if don't set this, will use the default normalizer from config
            /*"pre_tokenizer": [{"whitespace": {}}], // if don't set this, will use the default normalizer from config*/
            "pre_tokenizer": "default", // if don't set this, will use the default normalizer from config
            "post_processor": "default", // you cannot set this to None, this must be "default" or other post_processor(the tokenizers lib does not support remove the default post_processor)
            "prefix": "",   //the map target prefix
            "filed_map": { // this is the default value, you can provide other name
                "tokens": "tokens",
                "ids": "ids",
                "attention_mask": "attention_mask",
                "type_ids": "type_ids",
                "special_tokens_mask": "special_tokens_mask",
                "offsets": "offsets",
                "word_ids": "word_ids",
                "overflowing": "overflowing",
                "sequence_ids": "sequence_ids",
            }, // the tokenizer output(the key) map to the value
            "data_type": "*@*", // single or pair, if not provide, will calc by len(process_data)
            "process_data": "*@*",
            /*"data_type": "single", // single or pair, if not provide, will calc by len(process_data)*/
            /*"process_data": [*/
                /*["sentence", { "is_pretokenized": false}], */
            /*],*/
            /*"data_type": "pair", // single or pair*/
            /*"process_data": [*/
                /*['sentence_a', { "is_pretokenized": false}], */ 
                /*['sentence_b', {}], the config of the second data must as same as the first*/ 
            /*],*/
        },
        "predict": "train",
        "online": "train"
    }
}
