{
    "processor": {
        "_name": "basic@piece_rerank#pretrained",
        "config": {
            "feed_order": ["load", "piece_rerank_loader", "tokenizer", "piece_rerank_relabel", "save"]
            "tokenizer_config_path": "./data/bert/tokenizer.json", // the tokenizer config path (the tokenizer.json path)
            "data_dir": "./bert/output/",  // save load data base dir
        },
        "_link": {
            "config.tokenizer_config_path": [
                "subprocessor@tokenizer.config.train.config_path"
            ],
            "config.data_dir": [
                "subprocessor@load.config.base_dir",
                "subprocessor@save.config.base_dir"
            ],
        },
        "subprocessor@load": {
            "_base": "load",
        },
        "subprocessor@save": {
            "_base": "save",
            "config": {
                "base_dir": "",
                "train": {
                    "processed": "processed_data.pkl", # all data
                    "meta": {
                        "meta.pkl": [
                            "tokenizer"
                        ] #only for next time use
                    }
                },
                "predict": {
                    "processed": "processed_data.pkl",
                },
                "extend_train": {
                    "processed": "processed_data_extend.pkl",
                }
            }
        },
        "subprocessor@piece_rerank_relabel": {
            "_base": "piece_rerank_relabel",
            "config": {
                "train": {
                    "input_map": {  
                         "rank_info": "rank_info",
                    },
                    "output_map": {
                         "label_ids": "label_ids",
                    },
                },
            }
        },
        "subprocessor@piece_rerank_loader": { # the span_cls dataloader is the same as seq_lab
            "_base": "basic_data_loader@piece_rerank",
        },
        "subprocessor@tokenizer": {
            "_base": "fast_tokenizer",
            "config": {
                "train": {
                    "config_path": "*@*",
                    "deliver": "tokenizer",
                    "data_type": "single", # single or pair, if not provide, will calc by len(process_data)
                    "process_data": {
                        "is_pretokenized": true,
                    },
                    "truncation": {  # if this is set to null or empty, will not do trunc
                        "direction": "right", # default `right`, if set `left`, will reserve the rightest chars.
                        "stride": 128, # if the sequence is very long, will split to multiple span, stride is the window slide
                        "max_length": 384,
                        "strategy": "only_second", # Can be one of longest_first, only_first or only_second.
                    },
                    "input_map": {
                        "pretokenized_words": "pretokenized_words", #for pair inputs, tokenize the "sentence_a" && "sentence_b"
                        "pretokenized_word_offsets": "pretokenized_word_offsets"
                    },
                    "expand_examples": false, # if the sequence is very long, will split to multiple span, whether expand the examples
                    "fix_offset": false, # whether fix the offset for pretokenizerd word
                },
                "predict": "train",
                "online": "train"
            }
        },
    }
}
