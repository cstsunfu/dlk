{
    "root": {
        "_name": "pretrained_firstpiece_crf"
        "_link": {"config.meta_data": ['task.imodel.model.config.embedding_file', 'task.imodel.postprocessor.config.meta']},
        "_search": {},
        "config": {
            "save_dir": "output/benchmark/pretrained",  # must provide
            "data_path": "output/benchmark/pretrained/processed_data.pkl",  # must provide
            "meta_data": "output/benchmark/pretrained/meta.pkl",  # must provide
        },
        "task": {
            "manager": {
                "_base": "lightning",
                "config":{
                    "max_epochs": 4,
                    "gpus": 1,
                }
            },
            "imodel": {
                "_base": "basic@seq_lab#crf",
                "model": {
                    "_base": "basic@seq_lab#pretrained_transformers_crf",
                    "embedding": {
                        "_base": "pretrained_transformers@gather",
                        "module": {
                            "config": {
                                "pretrained_model_path": "*@*",
                            },
                            "_name": "bert",
                        },
                        "config": {
                            "pretrained_model_path": "*@*",
                            "embedding_dim": 768,
                            "input_map": {
                                "input_ids": "input_ids",
                                "attention_mask": "subword_mask",
                                "type_ids": "type_ids",
                                "gather_index": "gather_index",
                            },
                        },
                    },
                    "encoder": {
                        "_base": "lstm",
                        "config": {
                            "return_logits": "encoder_logits",
                            "output_map": {},
                            "input_size": 768,
                            "output_size": 768,
                            "num_layers": 1,
                            "dropout": 0.3, // dropout between layers
                        },
                    },
                    "config": {
                        "embedding_dim": 768,
                        "label_num": 11,
                        /*"pretrained_model_path": "./local_data/pretrained_model/roberta_base",*/
                        "pretrained_model_path": "./local_data/pretrained_model/bert_case",
                        "dropout": 0.3,
                    },
                    "_link": {
                        "config.pretrained_model_path": "embedding.config.pretrained_model_path"
                    },
                },
                "postprocessor": {
                    "config": {
                        "meta": '*@*',
                        "start_save_epoch": 20,
                    }
                },
                "scheduler": {
                    "_base": "linear_warmup"
                },
                "optimizer": {
                    "_base": "adamw@bias_nodecay",
                    "config": {
                        "lr": 3e-5,
                        "optimizer_special_groups":[
                        // special paramater groups set to special value, if some config key-value is not set, will use the default config in  optimizer_config.
                        // You should sort the config by priority(
                        //     e.g. the first group is ['linear.bias', {weight_decay: 0.1}], the second is [bias, [{weight_decay: 0.2}]], then the weight_decay of "*linea.bias*" will be 0.1, and the weight_decay of others *.bias.* will be 0.2
                            ["bias & LayerNorm.bias & LayerNorm.weight", {weight_decay: 0}],
                            ['crf & lstm', {"lr": 1e-3}],
                        ]
                    },
                },
                "postprocessor": {
                    "config": {
                        "meta": '*@*'
                        "use_crf": true, //use or not use crf
                        "word_ready": false, //already gather the subword first token as the word rep or not
                    }
                },
            },
            "datamodule": {
                "_base": "basic@seq_lab#wordmask",
                "config":{
                   "train_batch_size": 32,
                   "predict_batch_size": 32, //predict„ÄÅtest batch_size is equals to valid_batch_size
                }
            },
        },
    },
    "_focus": {
        "root._name": "task."
    },
}
