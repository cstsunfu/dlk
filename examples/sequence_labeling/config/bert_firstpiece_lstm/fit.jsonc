{
    "@fit": {
        "specific": {},
        "log_dir": "./logs",
        "processed_data_dir": "./data/processed_data",
        "@trainer@lightning": {
            "max_epochs": 8,
            "devices": "auto",
            "strategy": "auto",
            "accelerator": "cpu",
            "@callback@lr_monitor": {
                "logging_interval": "step"
            }
        },
        "@datamodule@default": {
            "train_batch_size": 32,
            "predict_batch_size": 64,
            "num_workers": -1,
            "pin_memory": true,
            "shuffle": true,
            "@dataset@default": {
                "repeat_for_valid": false,
                "key_type_pairs": {
                    "input_ids": "int",
                    "label_ids": "long",
                    "gather_index": "long",
                    "type_ids": "long",
                    "special_tokens_mask": "int"
                }
            },
            "@data_collate@default": {
                "gen_mask": {
                    "gather_index": "attention_mask",
                    "input_ids": "subword_mask"
                },
                "key_padding_pairs": {
                    "input_ids": 0,
                    "label_ids": -100,
                    "special_tokens_mask": 0
                }
            }
        },
        "@imodel@default": {
            "@model@basic": {
                "@initmethod@default": {},
                "@embedding@bert_like": {
                    "from_pretrain": true,
                    "pretrained_model_path": "./data/bert/",
                    "embedding_dim": 768,
                    "dropout": 0.1,
                    "input_map": {
                        "attention_mask": "subword_mask",
                        "gather_index": "gather_index"
                    }
                },
                "@encoder@lstm": {
                    "input_size": "@lambda @$$.@embedding.embedding_dim",
                    "output_size": "@lambda @$.input_size"
                },
                "@decoder@linear": {
                    "input_size": "@lambda @$$.@encoder.output_size",
                    "output_size": 11,
                    "pool": null
                }
            },
            "@scheduler@linear_warmup": {
                "num_warmup_steps": 0.1
            },
            "@loss@cross_entropy": {
                "pred_truth_pair": {
                    "logits": "label_ids"
                },
                "ignore_index": -100
            },
            "@optimizer@adamw": {
                "lr": 2e-5,
                "eps": 1e-08,
                "optimizer_special_groups": {
                    "order": [
                        "decoder",
                        "bias"
                    ], // the group order, if the para is in decoder & is in bias, set to decoder
                    "bias": {
                        "config": {
                            "weight_decay": 0
                        },
                        "pattern": [
                            "bias",
                            "LayerNorm\\.bias",
                            "LayerNorm\\.weight"
                        ]
                    },
                    "decoder": {
                        "config": {
                            "lr": 1e-3
                        },
                        "pattern": [
                            "crf",
                            "lstm"
                        ]
                    }
                }
            },
            "@postprocessor@seq_lab": {
                "label_vocab": "label_vocab.json",
                "word_ready": true,
                "tokenizer_path": "./data/bert/tokenizer.json"
            }
        }
    }
}
