{
  "version": "1.0",
  "truncation": null,
  "padding": null,
  "added_tokens": [
    {
      "id": 0,
      "special": true,
      "content": "[UNK]",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false
    },
    {
      "id": 1,
      "special": true,
      "content": "[CLS]",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false
    },
    {
      "id": 2,
      "special": true,
      "content": "[SEP]",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false
    },
    {
      "id": 3,
      "special": true,
      "content": "[PAD]",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false
    },
    {
      "id": 4,
      "special": true,
      "content": "[MASK]",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false
    }
  ],
  "normalizer": {
    "type": "Sequence",
    "normalizers": [
      {
        "type": "NFD"
      },
      {
        "type": "Lowercase"
      },
      {
        "type": "StripAccents"
      }
    ]
  },
  "pre_tokenizer": {
    "type": "Sequence",
    "pretokenizers": []
  },
  "post_processor": {
    "type": "TemplateProcessing",
    "single": [
      {
        "SpecialToken": {
          "id": "[CLS]",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "SpecialToken": {
          "id": "[SEP]",
          "type_id": 0
        }
      }
    ],
    "pair": [
      {
        "SpecialToken": {
          "id": "[CLS]",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "SpecialToken": {
          "id": "[SEP]",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "B",
          "type_id": 1
        }
      },
      {
        "SpecialToken": {
          "id": "[SEP]",
          "type_id": 1
        }
      }
    ],
    "special_tokens": {
      "[CLS]": {
        "id": "[CLS]",
        "ids": [
          1
        ],
        "tokens": [
          "[CLS]"
        ]
      },
      "[SEP]": {
        "id": "[SEP]",
        "ids": [
          2
        ],
        "tokens": [
          "[SEP]"
        ]
      }
    }
  },
  "decoder": null,
  "model": {
    "type": "BPE",
    "dropout": null,
    "unk_token": "[UNK]",
    "continuing_subword_prefix": null,
    "end_of_word_suffix": null,
    "fuse_unk": false,
    "vocab": {
      "[UNK]": 0,
      "[CLS]": 1,
      "[SEP]": 2,
      "[PAD]": 3,
      "[MASK]": 4,
      "\n": 5,
      " ": 6,
      "\"": 7,
      "#": 8,
      "$": 9,
      "&": 10,
      "'": 11,
      "(": 12,
      ")": 13,
      ",": 14,
      "-": 15,
      ".": 16,
      "/": 17,
      "0": 18,
      "1": 19,
      "2": 20,
      "3": 21,
      "4": 22,
      "5": 23,
      ":": 24,
      "=": 25,
      "[": 26,
      "]": 27,
      "_": 28,
      "`": 29,
      "a": 30,
      "b": 31,
      "c": 32,
      "d": 33,
      "e": 34,
      "f": 35,
      "g": 36,
      "h": 37,
      "i": 38,
      "j": 39,
      "k": 40,
      "l": 41,
      "m": 42,
      "n": 43,
      "o": 44,
      "p": 45,
      "r": 46,
      "s": 47,
      "t": 48,
      "u": 49,
      "v": 50,
      "w": 51,
      "x": 52,
      "y": 53,
      "z": 54,
      "{": 55,
      "}": 56,
      "、": 57,
      "  ": 58,
      "    ": 59,
      "        ": 60,
      "\":": 61,
      "\": ": 62,
      "                ": 63,
      "in": 64,
      ",\n": 65,
      "at": 66,
      "to": 67,
      "id": 68,
      "    \"": 69,
      "en": 70,
      "ken": 71,
      "token": 72,
      "\": \"": 73,
      ", ": 74,
      "be": 75,
      "dat": 76,
      "                \"": 77,
      "data": 78,
      "_id": 79,
      "ig": 80,
      "{\n": 81,
      "tr": 82,
      "                    \"": 83,
      "abe": 84,
      "labe": 85,
      "label": 86,
      "\",\n": 87,
      "on": 88,
      "ain": 89,
      "de": 90,
      "or": 91,
      "pr": 92,
      "train": 93,
      "\": [": 94,
      "_ids": 95,
      "],\n": 96,
      "_token": 97,
      "}\n": 98,
      "            ": 99,
      "\"、": 100,
      "//": 101,
      "_s": 102,
      "igin": 103,
      "origin": 104,
      "\"、\"": 105,
      "am": 106,
      "es": 107,
      "\": {\n": 108,
      "\"\n": 109,
      "``": 110,
      "ed": 111,
      ", //": 112,
      "ct": 113,
      "con": 114,
      "fig": 115,
      "ma": 116,
      "ing": 117,
      "config": 118,
      "\"train": 119,
      "et": 120,
      "ict": 121,
      "pred": 122,
      "ame": 123,
      "predict": 124,
      "./": 125,
      ".p": 126,
      "em": 127,
      "er": 128,
      "kl": 129,
      "s ": 130,
      "\": \"./": 131,
      "dev": 132,
      ".pkl": 133,
      "\", ": 134,
      "\"],\n": 135,
      "ces": 136,
      "dd": 137,
      "lu": 138,
      "oces": 139,
      "s\": [": 140,
      "tat": 141,
      "        \"": 142,
      "bedd": 143,
      "proces": 144,
      "\"、\"origin": 145,
      "embedd": 146,
      ".pkl\",\n": 147,
      "\", \"": 148,
      "embedding": 149,
      " t": 150,
      "li": 151,
      "},\n": 152,
      "}, //": 153,
      "token_ids": 154,
      "data_s": 155,
      "_tokens": 156,
      "_stat": 157,
      "map": 158,
      " th": 159,
      "data_set": 160,
      "_n": 161,
      "_map": 162,
      "e ": 163,
      "fr": 164,
      "ll": 165,
      "lin": 166,
      "us\": [": 167,
      "                }\n": 168,
      "                \"config": 169,
      "                \"_stat": 170,
      "                \"_n": 171,
      "label_ids": 172,
      "            {\n": 173,
      "            }, //": 174,
      "```": 175,
      "ame\": \"": 176,
      "us\": [\"train": 177,
      "                \"_status\": [\"train": 178,
      "                \"_name\": \"": 179,
      "', ": 180,
      "'train": 181,
      "'dev": 182,
      "_data": 183,
      "all": 184,
      "ns ": 185,
      "ver": 186,
      "y ": 187,
      "            \"": 188,
      "\": {": 189,
      "\": \"origin": 190,
      "                    \"de": 191,
      "                    \"data_set": 192,
      "onlin": 193,
      "\": ['train": 194,
      ", // ": 195,
      "process": 196,
      "liver": 197,
      "                \"config\": {\n": 198,
      "', 'dev": 199,
      "                    \"deliver": 200,
      "                    \"data_set\": ['train": 201,
      "online": 202,
      "', 'dev'": 203,
      "                    \"data_set\": ['train', 'dev'": 204,
      "\"u": 205,
      "..": 206,
      ".data": 207,
      ".config": 208,
      "][": 209,
      "co": 210,
      "clu": 211,
      "d.data": 212,
      "e\n": 213,
      "mns ": 214,
      "pd.data": 215,
      "re": 216,
      "uid": 217,
      "\": pd.data": 218,
      "inclu": 219,
      "de th": 220,
      "\"、\"label": 221,
      "ese ": 222,
      "may ": 223,
      "ame, // ": 224,
      "predict\", \"": 225,
      "lumns ": 226,
      "\"、\"origin\"、\"label": 227,
      "\", \"predict\", \"": 228,
      "_map\": {": 229,
      "frame, // ": 230,
      "                \"_status\": [\"train\", \"predict\", \"": 231,
      "process.": 232,
      "online\"],\n": 233,
      "                    \"data_set\": ['train', 'dev'],\n": 234,
      "\"uuid": 235,
      ".config.": 236,
      "columns ": 237,
      "\": pd.dataframe, // ": 238,
      "include th": 239,
      "ese columns ": 240,
      "may include th": 241,
      "_map\": {\"": 242,
      "                \"_status\": [\"train\", \"predict\", \"online\"],\n": 243,
      "\"uuid\"、\"origin\"、\"label": 244,
      "\": pd.dataframe, // may include th": 245,
      "ese columns \"uuid\"、\"origin\"、\"label": 246,
      "\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label": 247,
      " e": 248,
      " f": 249,
      "\" ": 250,
      "\"},\n": 251,
      "# ": 252,
      "## ": 253,
      "gat": 254,
      "her": 255,
      "js": 256,
      "le\n": 257,
      "pa": 258,
      "ple\n": 259,
      "str": 260,
      "xam": 261,
      "    }\n": 262,
      "to_data": 263,
      "token_": 264,
      "tokens": 265,
      "\": \"label": 266,
      "                \"data": 267,
      "on\n": 268,
      "_token_ids": 269,
      "```\n": 270,
      "```js": 271,
      "                \"_name\": \"token_": 272,
      "                    \"deliver\": \"": 273,
      " exam": 274,
      "gather": 275,
      "string": 276,
      "                \"data.": 277,
      "```json\n": 278,
      " example\n": 279,
      " or": 280,
      " li": 281,
      "\"label": 282,
      "\"process.": 283,
      "$re": 284,
      "'data": 285,
      "'][": 286,
      "2.config.": 287,
      "[..": 288,
      "['data": 289,
      "]][": 290,
      "_c": 291,
      "_m": 292,
      "_label": 293,
      "_embedding": 294,
      "_pa": 295,
      "ar": 296,
      "av": 297,
      "d\": \"": 298,
      "f,\n": 299,
      "ho": 300,
      "har": 301,
      "ir": 302,
      "is ": 303,
      "mat": 304,
      "m_data": 305,
      "om_data": 306,
      "pu": 307,
      "st": 308,
      "s\"、\"origin": 309,
      "sav": 310,
      "t['data": 311,
      "us": 312,
      "        \"train": 313,
      "        },\n": 314,
      "\":{\n": 315,
      "\":\"process.": 316,
      "\": $re": 317,
      "                        \"": 318,
      "inpu": 319,
      "to_id": 320,
      "    \"id": 321,
      "    \"data": 322,
      "\": \"id": 323,
      "                \"embedding": 324,
      "                \"token_ids": 325,
      "                \"label_ids": 326,
      "data f": 327,
      "data_pa": 328,
      "_idx": 329,
      "_id_map\": {\"": 330,
      "tru": 331,
      "                    \"tokens": 332,
      "                    \"data_pa": 333,
      "ormat": 334,
      "\": [\"label": 335,
      "_token_idx": 336,
      "            }\n": 337,
      "// ": 338,
      "\"、\"label_ids": 339,
      "etho": 340,
      "\": \"./embedding": 341,
      "\": \"./token_ids": 342,
      "\": \"./label_ids": 343,
      "dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label": 344,
      "        \"dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label": 345,
      "process ": 346,
      "\"、\"origin_token_ids": 347,
      "_tokens\",\n": 348,
      "_tokens\"、\"label_ids": 349,
      "data_set[..": 350,
      "from_data": 351,
      "                \"_status\": [\"train\"],\n": 352,
      "            \"predict": 353,
      "            \"process.": 354,
      "                    \"deliver_m": 355,
      "..],\n": 356,
      "_map\": {\"id": 357,
      "                \"_name\": \"token_gather": 358,
      "                    \"deliver\": \"all": 359,
      "string or": 360,
      " list": 361,
      "'][data_set[..": 362,
      "_char": 363,
      "ir\": {\n": 364,
      "s\"、\"origin_tokens\"、\"label_ids": 365,
      "save": 366,
      "t['data'][data_set[..": 367,
      "        \"train\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label": 368,
      "\": $ref,\n": 369,
      "input['data'][data_set[..": 370,
      "to_id\",\n": 371,
      "    \"data\": {\n": 372,
      "                \"embedding\": \"./embedding": 373,
      "                \"token_ids\": \"./token_ids": 374,
      "                \"label_ids\": \"./label_ids": 375,
      "data format": 376,
      "                    \"data_pair\": {\n": 377,
      "ethod\": \"": 378,
      "\"、\"origin_token_ids\"\n": 379,
      "                    \"deliver_method\": \"": 380,
      "string or list": 381,
      "_char_token_idx": 382,
      "s\"、\"origin_tokens\"、\"label_ids\"、\"origin_token_ids\"\n": 383,
      "                \"embedding\": \"./embedding.pkl\",\n": 384,
      "                \"token_ids\": \"./token_ids.pkl\",\n": 385,
      "                \"label_ids\": \"./label_ids.pkl\",\n": 386,
      "data format example\n": 387,
      "string or list, ": 388,
      " \"": 389,
      " d": 390,
      " n": 391,
      " all": 392,
      " // ": 393,
      " process ": 394,
      " data format example\n": 395,
      " string or list, ": 396,
      "\"}\n": 397,
      "\", // ": 398,
      "& \"": 399,
      "(input['data'][data_set[..": 400,
      ")\n": 401,
      "-1": 402,
      ".predict": 403,
      ".online": 404,
      "0 th": 405,
      "0.config.": 406,
      "1\n": 407,
      "2\n": 408,
      "3\n": 409,
      "4\n": 410,
      "5\n": 411,
      "=f": 412,
      "[input['data'][data_set[..": 413,
      "]\n": 414,
      "]]][": 415,
      "])\n": 416,
      "]=f": 417,
      "_lin": 418,
      "_to_id\",\n": 419,
      "ad": 420,
      "ans ": 421,
      "ce": 422,
      "cre": 423,
      "c_embedding": 424,
      "ew": 425,
      "e_map": 426,
      "eans ": 427,
      "f th": 428,
      "get": 429,
      "il": 430,
      "is": 431,
      "iz": 432,
      "ill": 433,
      "ic_embedding": 434,
      "if th": 435,
      "k\": {\n": 436,
      "lo": 437,
      "m\n": 438,
      "means ": 439,
      "nu": 440,
      "n(input['data'][data_set[..": 441,
      "new": 442,
      "ot": 443,
      "o th": 444,
      "pt": 445,
      "res": 446,
      "s\",\n": 447,
      "stat": 448,
      "s\": \"origin": 449,
      "spa": 450,
      "ure": 451,
      "will": 452,
      "    {\n": 453,
      "    },\n": 454,
      "    }, //": 455,
      "        \n": 456,
      "        ],\n": 457,
      "        }\n": 458,
      "\": tru": 459,
      "                    }\n": 460,
      "                    }, //": 461,
      "ate_map": 462,
      "to process ": 463,
      "ids\": \"origin": 464,
      "    \"token": 465,
      "    \"label": 466,
      "    \"embedding": 467,
      "    \"all": 468,
      "token\": \"origin": 469,
      "token\"},\n": 470,
      "token\": \"id": 471,
      "\": \"token\"},\n": 472,
      ", us": 473,
      ", will": 474,
      "_id\"\n": 475,
      "_id\"],\n": 476,
      "_id\": tru": 477,
      "                    \"token_ids": 478,
      "                    \"map": 479,
      "                    \"all": 480,
      "                    \"to_data": 481,
      "                    \"from_data": 482,
      "                    \"res": 483,
      "                    \"ids\": \"origin": 484,
      "                    \"token\": \"origin": 485,
      "label\": \"label": 486,
      "label\": \"id": 487,
      "label_to_id\",\n": 488,
      "train\": \"./": 489,
      "train.pkl\",\n": 490,
      "train\": {": 491,
      "\": [..],\n": 492,
      "_tokens\": [": 493,
      "_token_map\": {\"id": 494,
      "_tokeniz": 495,
      "            },\n": 496,
      "origin\": \"origin": 497,
      "ed\n": 498,
      "ed data format example\n": 499,
      "cture": 500,
      "config example\n": 501,
      "\"train\" ": 502,
      "\"train.predict": 503,
      "predict\"\n": 504,
      "predict\": \"./": 505,
      "predict.pkl": 506,
      "empt": 507,
      "er\"\n": 508,
      "\": \"./dev": 509,
      "dev\": \"./dev": 510,
      "s\": [\n": 511,
      "        \"proces": 512,
      "        \"save": 513,
      "        \"_lin": 514,
      "        \"lo": 515,
      "embedding\",\n": 516,
      "token_ids\",\n": 517,
      "token_ids\":\"process.": 518,
      "_tokens\"\n": 519,
      "_tokens\":\"process.": 520,
      "_tokens\": $ref,\n": 521,
      "_static_embedding": 522,
      "_map\" ": 523,
      "e process ": 524,
      "e \"train.predict": 525,
      "                \"config\":{\n": 526,
      "            }, //0 th": 527,
      "            }, //1\n": 528,
      "            }, //2\n": 529,
      "            }, //3\n": 530,
      "            }, //4\n": 531,
      "            }, //5\n": 532,
      "                \"_name\": \"get": 533,
      "                \"_name\": \"spa": 534,
      "                \"_name\": \"label_to_id\",\n": 535,
      "y string": 536,
      "            \"train\": {": 537,
      "\": \"origin_char_token_idx": 538,
      "\": \"origin\", // ": 539,
      "processed data format example\n": 540,
      "                    \"data_set\": ['train', 'dev']\n": 541,
      ".config.token_ids\",\n": 542,
      " fil": 543,
      "\" means ": 544,
      "## to process ": 545,
      "## config example\n": 546,
      "## processed data format example\n": 547,
      "to_data[input['data'][data_set[..": 548,
      "to_data]=f": 549,
      "tokens\",\n": 550,
      "\": \"label\",\n": 551,
      "\": \"label\"},\n": 552,
      "_token_ids\",\n": 553,
      "                \"_name\": \"token_to_id\",\n": 554,
      "                    \"deliver\": \"embedding\",\n": 555,
      "gather all": 556,
      "                \"data.train\": \"./": 557,
      "                \"data.predict\": \"./": 558,
      "                \"data.dev\": \"./dev": 559,
      "2.config.token_ids\":\"process.": 560,
      "2.config.tokens\",\n": 561,
      "]][from_data": 562,
      "_label_map\": {\"id": 563,
      "_labels\",\n": 564,
      "_embedding_id\": tru": 565,
      "is is ": 566,
      "us, us": 567,
      "                        \"label\": \"label": 568,
      "                        \"origin\": \"origin": 569,
      "    \"id_token_map\": {\"id": 570,
      "    \"id_label_map\": {\"id": 571,
      "_id_map\": {\"token\": \"id": 572,
      "_id_map\": {\"label\": \"id": 573,
      "tructure": 574,
      "                    \"tokens\": \"origin\", // ": 575,
      "                    \"tokens\": \"label\",\n": 576,
      "\": [\"label\"],\n": 577,
      "\": [\"label_id\"],\n": 578,
      "// if th": 579,
      "        \"dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label\"\n": 580,
      "        \"dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"labels\"、\"origin_tokens\"、\"label_ids\"、\"origin_token_ids\"\n": 581,
      "            \"predict\": {\n": 582,
      "            \"predict.online": 583,
      "            \"process.0.config.": 584,
      "            \"process.2.config.token_ids\":\"process.": 585,
      "                \"_name\": \"token_gather\",\n": 586,
      "                \"_name\": \"token_gather\"\n": 587,
      "                    \"deliver\": \"all_tokens\",\n": 588,
      "                    \"deliver\": \"all_labels\",\n": 589,
      "save_tokens\":\"process.": 590,
      "        \"train\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label\"\n": 591,
      "        \"train\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"labels\"、\"origin_tokens\"、\"label_ids\"、\"origin_token_ids\"\n": 592,
      "                    \"deliver_method\": \"cre": 593,
      "                    \"deliver_method\": \"new": 594,
      "_char_token_idx\": \"origin_char_token_idx": 595,
      "string or list, gather all": 596,
      " do th": 597,
      " not": 598,
      " // stat": 599,
      " string or list, to_data[input['data'][data_set[..": 600,
      "& \"predict\"\n": 601,
      "-1.config.token_ids\",\n": 602,
      "]]][to_data]=f": 603,
      "ad\": {\n": 604,
      "ce_tokeniz": 605,
      "is\n": 606,
      "num\n": 607,
      "n(input['data'][data_set[..]][from_data": 608,
      "                    }, // string or list, to_data[input['data'][data_set[..": 609,
      "ate_map\"\n": 610,
      "    \"token_id_map\": {\"token\": \"id": 611,
      "    \"label_id_map\": {\"label\": \"id": 612,
      "    \"embedding\": [..],\n": 613,
      "    \"all_tokens\": [": 614,
      ", will not": 615,
      "                    \"token_ids\": $ref,\n": 616,
      "                    \"map_char_token_idx\": \"origin_char_token_idx": 617,
      "                    \"all_tokens\": $ref,\n": 618,
      "                    \"to_data\": [\"label_id\"],\n": 619,
      "                    \"from_data\": [\"label\"],\n": 620,
      "                    \"restructure": 621,
      "                    \"ids\": \"origin_token_ids\",\n": 622,
      "                    \"token\": \"origin_tokens\",\n": 623,
      "\"train\" & \"predict\"\n": 624,
      "predict.pkl\"\n": 625,
      "empty string": 626,
      "        \"process\": [\n": 627,
      "        \"save\": {\n": 628,
      "        \"_link\": {\n": 629,
      "        \"load\": {\n": 630,
      "_static_embedding\",\n": 631,
      "_map\" // if th": 632,
      "e process num\n": 633,
      "e \"train.predict\" means ": 634,
      "            }, //0 the process num\n": 635,
      "                \"_name\": \"get_static_embedding\",\n": 636,
      "                \"_name\": \"space_tokeniz": 637,
      "            \"train\": { // stat": 638,
      " filed\n": 639,
      "## to process data format example\n": 640,
      "                \"data.train\": \"./train.pkl\",\n": 641,
      "                \"data.predict\": \"./predict.pkl\"\n": 642,
      "                \"data.dev\": \"./dev.pkl\",\n": 643,
      "_embedding_id\": true\n": 644,
      "is is empty string": 645,
      "us, use \"train.predict\" means ": 646,
      "                        \"label\": \"label_id\"\n": 647,
      "                        \"origin\": \"origin_tokens\"\n": 648,
      "    \"id_token_map\": {\"id\": \"token\"},\n": 649,
      "    \"id_label_map\": {\"id\": \"label\"},\n": 650,
      "                    \"tokens\": \"origin\", // string or list, gather all": 651,
      "            \"predict.online\":{\n": 652,
      "            \"process.0.config.save_tokens\":\"process.": 653,
      "            \"process.2.config.token_ids\":\"process.-1.config.token_ids\",\n": 654,
      "                    \"deliver_method\": \"create_map\"\n": 655,
      "                    \"deliver_method\": \"new\"\n": 656,
      " do this\n": 657,
      "]]][to_data]=fn(input['data'][data_set[..]][from_data": 658,
      "                    }, // string or list, to_data[input['data'][data_set[..]]][to_data]=fn(input['data'][data_set[..]][from_data": 659,
      "    \"token_id_map\": {\"token\": \"id\"},\n": 660,
      "    \"label_id_map\": {\"label\": \"id\"}\n": 661,
      "    \"all_tokens\": [..],\n": 662,
      ", will not do this\n": 663,
      "                    \"map_char_token_idx\": \"origin_char_token_idx_map\" // if th": 664,
      "                    \"restructure_embedding_id\": true\n": 665,
      "                \"_name\": \"space_tokenizer\"\n": 666,
      "            \"train\": { // status, use \"train.predict\" means ": 667,
      "is is empty string, will not do this\n": 668,
      "                    \"tokens\": \"origin\", // string or list, gather all filed\n": 669,
      "            \"process.0.config.save_tokens\":\"process.2.config.tokens\",\n": 670,
      "                    }, // string or list, to_data[input['data'][data_set[..]]][to_data]=fn(input['data'][data_set[..]][from_data])\n": 671,
      "                    \"map_char_token_idx\": \"origin_char_token_idx_map\" // if this is empty string, will not do this\n": 672,
      "            \"train\": { // status, use \"train.predict\" means \"train\" & \"predict\"\n": 673
    },
    "merges": [
      "   ",
      "     ",
      "         ",
      "\" :",
      "\":  ",
      "                 ",
      "i n",
      ", \n",
      "a t",
      "t o",
      "i d",
      "     \"",
      "e n",
      "k en",
      "to ken",
      "\":  \"",
      ",  ",
      "b e",
      "d at",
      "                 \"",
      "dat a",
      "_ id",
      "i g",
      "{ \n",
      "t r",
      "                     \"",
      "a be",
      "l abe",
      "labe l",
      "\" ,\n",
      "o n",
      "a in",
      "d e",
      "o r",
      "p r",
      "tr ain",
      "\":  [",
      "_id s",
      "] ,\n",
      "_ token",
      "} \n",
      "             ",
      "\" 、",
      "/ /",
      "_ s",
      "ig in",
      "or igin",
      "\"、 \"",
      "a m",
      "e s",
      "\":  {\n",
      "\" \n",
      "` `",
      "e d",
      ",  //",
      "c t",
      "c on",
      "f ig",
      "m a",
      "in g",
      "con fig",
      "\" train",
      "e t",
      "i ct",
      "pr ed",
      "am e",
      "pred ict",
      ". /",
      ". p",
      "e m",
      "e r",
      "k l",
      "s  ",
      "\": \" ./",
      "de v",
      ".p kl",
      "\" , ",
      "\" ],\n",
      "c es",
      "d d",
      "l u",
      "o ces",
      "s \": [",
      "t at",
      "         \"",
      "be dd",
      "pr oces",
      "\"、\" origin",
      "em bedd",
      ".pkl \",\n",
      "\",  \"",
      "embedd ing",
      "  t",
      "l i",
      "} ,\n",
      "} , //",
      "token _ids",
      "data _s",
      "_token s",
      "_s tat",
      "ma p",
      " t h",
      "data_s et",
      "_ n",
      "_ map",
      "e  ",
      "f r",
      "l l",
      "l in",
      "u s\": [",
      "                 }\n",
      "                \" config",
      "                \" _stat",
      "                \" _n",
      "label _ids",
      "             {\n",
      "             }, //",
      "`` `",
      "ame \": \"",
      "us\": [ \"train",
      "                \"_stat us\": [\"train",
      "                \"_n ame\": \"",
      "' , ",
      "' train",
      "' dev",
      "_ data",
      "a ll",
      "n s ",
      "v er",
      "y  ",
      "             \"",
      "\":  {",
      "\": \" origin",
      "                    \" de",
      "                    \" data_set",
      "on lin",
      "\": [ 'train",
      ", //  ",
      "proces s",
      "li ver",
      "                \"config \": {\n",
      "',  'dev",
      "                    \"de liver",
      "                    \"data_set \": ['train",
      "onlin e",
      "', 'dev '",
      "                    \"data_set\": ['train ', 'dev'",
      "\" u",
      ". .",
      ". data",
      ". config",
      "] [",
      "c o",
      "c lu",
      "d .data",
      "e \n",
      "m ns ",
      "p d.data",
      "r e",
      "u id",
      "\":  pd.data",
      "in clu",
      "de  th",
      "\"、\" label",
      "es e ",
      "ma y ",
      "ame , // ",
      "predict \", \"",
      "lu mns ",
      "\"、\"origin \"、\"label",
      "\", \" predict\", \"",
      "_map \": {",
      "fr ame, // ",
      "                \"_status\": [\"train \", \"predict\", \"",
      "process .",
      "online \"],\n",
      "                    \"data_set\": ['train', 'dev' ],\n",
      "\"u uid",
      ".config .",
      "co lumns ",
      "\": pd.data frame, // ",
      "inclu de th",
      "ese  columns ",
      "may  include th",
      "_map\": { \"",
      "                \"_status\": [\"train\", \"predict\", \" online\"],\n",
      "\"uuid \"、\"origin\"、\"label",
      "\": pd.dataframe, //  may include th",
      "ese columns  \"uuid\"、\"origin\"、\"label",
      "\": pd.dataframe, // may include th ese columns \"uuid\"、\"origin\"、\"label",
      "  e",
      "  f",
      "\"  ",
      "\" },\n",
      "#  ",
      "# # ",
      "g at",
      "h er",
      "j s",
      "l e\n",
      "p a",
      "p le\n",
      "s tr",
      "x am",
      "     }\n",
      "to _data",
      "token _",
      "token s",
      "\": \" label",
      "                \" data",
      "on \n",
      "_token _ids",
      "``` \n",
      "``` js",
      "                \"_name\": \" token_",
      "                    \"deliver \": \"",
      " e xam",
      "gat her",
      "str ing",
      "                \"data .",
      "```js on\n",
      " exam ple\n",
      "  or",
      "  li",
      "\" label",
      "\" process.",
      "$ re",
      "' data",
      "' ][",
      "2 .config.",
      "[ ..",
      "[ 'data",
      "] ][",
      "_ c",
      "_ m",
      "_ label",
      "_ embedding",
      "_ pa",
      "a r",
      "a v",
      "d \": \"",
      "f ,\n",
      "h o",
      "h ar",
      "i r",
      "i s ",
      "m at",
      "m _data",
      "o m_data",
      "p u",
      "s t",
      "s \"、\"origin",
      "s av",
      "t ['data",
      "u s",
      "         \"train",
      "         },\n",
      "\": {\n",
      "\": \"process.",
      "\":  $re",
      "                         \"",
      "in pu",
      "to _id",
      "    \" id",
      "    \" data",
      "\": \" id",
      "                \" embedding",
      "                \" token_ids",
      "                \" label_ids",
      "data  f",
      "data _pa",
      "_id x",
      "_id _map\": {\"",
      "tr u",
      "                    \" tokens",
      "                    \" data_pa",
      "or mat",
      "\": [ \"label",
      "_token _idx",
      "             }\n",
      "//  ",
      "\"、\" label_ids",
      "et ho",
      "\": \"./ embedding",
      "\": \"./ token_ids",
      "\": \"./ label_ids",
      "dev \": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label",
      "        \" dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label",
      "proces s ",
      "\"、\"origin _token_ids",
      "_tokens \",\n",
      "_tokens \"、\"label_ids",
      "data_set [..",
      "fr om_data",
      "                \"_status\": [\"train \"],\n",
      "            \" predict",
      "            \" process.",
      "                    \"deliver _m",
      ".. ],\n",
      "_map\": {\" id",
      "                \"_name\": \"token_ gather",
      "                    \"deliver\": \" all",
      "string  or",
      " li st",
      "'][ data_set[..",
      "_c har",
      "ir \": {\n",
      "s\"、\"origin _tokens\"、\"label_ids",
      "sav e",
      "t['data '][data_set[..",
      "        \"train \": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label",
      "\": $re f,\n",
      "inpu t['data'][data_set[..",
      "to_id \",\n",
      "    \"data \": {\n",
      "                \"embedding \": \"./embedding",
      "                \"token_ids \": \"./token_ids",
      "                \"label_ids \": \"./label_ids",
      "data f ormat",
      "                    \"data_pa ir\": {\n",
      "etho d\": \"",
      "\"、\"origin_token_ids \"\n",
      "                    \"deliver_m ethod\": \"",
      "string or  list",
      "_char _token_idx",
      "s\"、\"origin_tokens\"、\"label_ids \"、\"origin_token_ids\"\n",
      "                \"embedding\": \"./embedding .pkl\",\n",
      "                \"token_ids\": \"./token_ids .pkl\",\n",
      "                \"label_ids\": \"./label_ids .pkl\",\n",
      "data format  example\n",
      "string or list , ",
      "  \"",
      "  d",
      "  n",
      "  all",
      "  // ",
      "  process ",
      "  data format example\n",
      "  string or list, ",
      "\" }\n",
      "\" , // ",
      "&  \"",
      "( input['data'][data_set[..",
      ") \n",
      "- 1",
      ". predict",
      ". online",
      "0  th",
      "0 .config.",
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "= f",
      "[ input['data'][data_set[..",
      "] \n",
      "] ]][",
      "] )\n",
      "] =f",
      "_ lin",
      "_ to_id\",\n",
      "a d",
      "a ns ",
      "c e",
      "c re",
      "c _embedding",
      "e w",
      "e _map",
      "e ans ",
      "f  th",
      "g et",
      "i l",
      "i s",
      "i z",
      "i ll",
      "i c_embedding",
      "i f th",
      "k \": {\n",
      "l o",
      "m \n",
      "m eans ",
      "n u",
      "n (input['data'][data_set[..",
      "n ew",
      "o t",
      "o  th",
      "p t",
      "r es",
      "s \",\n",
      "s tat",
      "s \": \"origin",
      "s pa",
      "u re",
      "w ill",
      "     {\n",
      "     },\n",
      "     }, //",
      "         \n",
      "         ],\n",
      "         }\n",
      "\":  tru",
      "                     }\n",
      "                     }, //",
      "at e_map",
      "to  process ",
      "id s\": \"origin",
      "    \" token",
      "    \" label",
      "    \" embedding",
      "    \" all",
      "token \": \"origin",
      "token \"},\n",
      "token \": \"id",
      "\": \" token\"},\n",
      ",  us",
      ",  will",
      "_id \"\n",
      "_id \"],\n",
      "_id \": tru",
      "                    \" token_ids",
      "                    \" map",
      "                    \" all",
      "                    \" to_data",
      "                    \" from_data",
      "                    \" res",
      "                    \" ids\": \"origin",
      "                    \" token\": \"origin",
      "label \": \"label",
      "label \": \"id",
      "label _to_id\",\n",
      "train \": \"./",
      "train .pkl\",\n",
      "train \": {",
      "\": [ ..],\n",
      "_token s\": [",
      "_token _map\": {\"id",
      "_token iz",
      "             },\n",
      "origin \": \"origin",
      "ed \n",
      "ed  data format example\n",
      "ct ure",
      "config  example\n",
      "\"train \" ",
      "\"train .predict",
      "predict \"\n",
      "predict \": \"./",
      "predict .pkl",
      "em pt",
      "er \"\n",
      "\": \"./ dev",
      "dev \": \"./dev",
      "s\": [ \n",
      "        \" proces",
      "        \" save",
      "        \" _lin",
      "        \" lo",
      "embedding \",\n",
      "token_ids \",\n",
      "token_ids \":\"process.",
      "_tokens \"\n",
      "_tokens \":\"process.",
      "_tokens \": $ref,\n",
      "_stat ic_embedding",
      "_map \" ",
      "e  process ",
      "e  \"train.predict",
      "                \"config \":{\n",
      "            }, // 0 th",
      "            }, // 1\n",
      "            }, // 2\n",
      "            }, // 3\n",
      "            }, // 4\n",
      "            }, // 5\n",
      "                \"_name\": \" get",
      "                \"_name\": \" spa",
      "                \"_name\": \" label_to_id\",\n",
      "y  string",
      "            \" train\": {",
      "\": \"origin _char_token_idx",
      "\": \"origin \", // ",
      "process ed data format example\n",
      "                    \"data_set\": ['train', 'dev' ]\n",
      ".config. token_ids\",\n",
      " f il",
      "\"  means ",
      "##  to process ",
      "##  config example\n",
      "##  processed data format example\n",
      "to_data [input['data'][data_set[..",
      "to_data ]=f",
      "tokens \",\n",
      "\": \"label \",\n",
      "\": \"label \"},\n",
      "_token_ids \",\n",
      "                \"_name\": \"token_ to_id\",\n",
      "                    \"deliver\": \" embedding\",\n",
      "gather  all",
      "                \"data. train\": \"./",
      "                \"data. predict\": \"./",
      "                \"data. dev\": \"./dev",
      "2.config. token_ids\":\"process.",
      "2.config. tokens\",\n",
      "]][ from_data",
      "_label _map\": {\"id",
      "_label s\",\n",
      "_embedding _id\": tru",
      "is  is ",
      "us , us",
      "                        \" label\": \"label",
      "                        \" origin\": \"origin",
      "    \"id _token_map\": {\"id",
      "    \"id _label_map\": {\"id",
      "_id_map\": {\" token\": \"id",
      "_id_map\": {\" label\": \"id",
      "tru cture",
      "                    \"tokens \": \"origin\", // ",
      "                    \"tokens \": \"label\",\n",
      "\": [\"label \"],\n",
      "\": [\"label _id\"],\n",
      "//  if th",
      "        \"dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label \"\n",
      "        \"dev\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label s\"、\"origin_tokens\"、\"label_ids\"、\"origin_token_ids\"\n",
      "            \"predict \": {\n",
      "            \"predict .online",
      "            \"process. 0.config.",
      "            \"process. 2.config.token_ids\":\"process.",
      "                \"_name\": \"token_gather \",\n",
      "                \"_name\": \"token_gather \"\n",
      "                    \"deliver\": \"all _tokens\",\n",
      "                    \"deliver\": \"all _labels\",\n",
      "save _tokens\":\"process.",
      "        \"train\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label \"\n",
      "        \"train\": pd.dataframe, // may include these columns \"uuid\"、\"origin\"、\"label s\"、\"origin_tokens\"、\"label_ids\"、\"origin_token_ids\"\n",
      "                    \"deliver_method\": \" cre",
      "                    \"deliver_method\": \" new",
      "_char_token_idx \": \"origin_char_token_idx",
      "string or list,  gather all",
      " d o th",
      " n ot",
      " //  stat",
      " string or list,  to_data[input['data'][data_set[..",
      "& \" predict\"\n",
      "-1 .config.token_ids\",\n",
      "]]][ to_data]=f",
      "ad \": {\n",
      "ce _tokeniz",
      "is \n",
      "nu m\n",
      "n(input['data'][data_set[.. ]][from_data",
      "                    }, //  string or list, to_data[input['data'][data_set[..",
      "ate_map \"\n",
      "    \"token _id_map\": {\"token\": \"id",
      "    \"label _id_map\": {\"label\": \"id",
      "    \"embedding \": [..],\n",
      "    \"all _tokens\": [",
      ", will  not",
      "                    \"token_ids \": $ref,\n",
      "                    \"map _char_token_idx\": \"origin_char_token_idx",
      "                    \"all _tokens\": $ref,\n",
      "                    \"to_data \": [\"label_id\"],\n",
      "                    \"from_data \": [\"label\"],\n",
      "                    \"res tructure",
      "                    \"ids\": \"origin _token_ids\",\n",
      "                    \"token\": \"origin _tokens\",\n",
      "\"train\"  & \"predict\"\n",
      "predict.pkl \"\n",
      "empt y string",
      "        \"proces s\": [\n",
      "        \"save \": {\n",
      "        \"_lin k\": {\n",
      "        \"lo ad\": {\n",
      "_static_embedding \",\n",
      "_map\"  // if th",
      "e process  num\n",
      "e \"train.predict \" means ",
      "            }, //0 th e process num\n",
      "                \"_name\": \"get _static_embedding\",\n",
      "                \"_name\": \"spa ce_tokeniz",
      "            \"train\": {  // stat",
      " fil ed\n",
      "## to process  data format example\n",
      "                \"data.train\": \"./ train.pkl\",\n",
      "                \"data.predict\": \"./ predict.pkl\"\n",
      "                \"data.dev\": \"./dev .pkl\",\n",
      "_embedding_id\": tru e\n",
      "is is  empty string",
      "us, us e \"train.predict\" means ",
      "                        \"label\": \"label _id\"\n",
      "                        \"origin\": \"origin _tokens\"\n",
      "    \"id_token_map\": {\"id \": \"token\"},\n",
      "    \"id_label_map\": {\"id \": \"label\"},\n",
      "                    \"tokens\": \"origin\", //  string or list, gather all",
      "            \"predict.online \":{\n",
      "            \"process.0.config. save_tokens\":\"process.",
      "            \"process.2.config.token_ids\":\"process. -1.config.token_ids\",\n",
      "                    \"deliver_method\": \"cre ate_map\"\n",
      "                    \"deliver_method\": \"new \"\n",
      " do th is\n",
      "]]][to_data]=f n(input['data'][data_set[..]][from_data",
      "                    }, // string or list, to_data[input['data'][data_set[.. ]]][to_data]=fn(input['data'][data_set[..]][from_data",
      "    \"token_id_map\": {\"token\": \"id \"},\n",
      "    \"label_id_map\": {\"label\": \"id \"}\n",
      "    \"all_tokens\": [ ..],\n",
      ", will not  do this\n",
      "                    \"map_char_token_idx\": \"origin_char_token_idx _map\" // if th",
      "                    \"restructure _embedding_id\": true\n",
      "                \"_name\": \"space_tokeniz er\"\n",
      "            \"train\": { // stat us, use \"train.predict\" means ",
      "is is empty string , will not do this\n",
      "                    \"tokens\": \"origin\", // string or list, gather all  filed\n",
      "            \"process.0.config.save_tokens\":\"process. 2.config.tokens\",\n",
      "                    }, // string or list, to_data[input['data'][data_set[..]]][to_data]=fn(input['data'][data_set[..]][from_data ])\n",
      "                    \"map_char_token_idx\": \"origin_char_token_idx_map\" // if th is is empty string, will not do this\n",
      "            \"train\": { // status, use \"train.predict\" means  \"train\" & \"predict\"\n"
    ]
  }
}