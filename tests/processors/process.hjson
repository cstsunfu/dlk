{
    "load": {
        "predict&online":{
            "meta_data": "./meta.pkl" // all of token_ids embedding, label_ids etc.
            "token_ids": "./token_ids.pkl",
            "embedding": "./embedding.pkl",
            "label_ids": "./label_ids.pkl",
        }
    },
    "process": [
        {
            '_name': 'wordpiece_tokenizer'
            '_status': ['train', 'predict', 'online'],
            'config': {
                'post_processor': 'bert', // if don't set this, will use the default normalizer from config, WARNING: not support disable  the default setting( so the default tokenizer.post_tokenizer should be null and setting in this configure)
                'config_path': './token.json',
                "pre_tokenizer": ["whitespace": {}], // if don't set this, will use the default normalizer from config
                "config_path": "./token.json",
                "normalizer": ['nfd', 'lowercase', 'strip_accents', "some_processor_need_config": {config}], // if don't set this, will use the default normalizer from config
                'data_set': {                   // for different status, this processor will process different part of data
                    'train': ['train', 'dev'],
                    'predict': ['predict'],
                    'online': ['online']
                },
                "filed_map": {
                    "tokens": "tokens",
                    "ids": "ids",
                    "attention_mask": "attention_mask",
                    "type_ids": "type_ids",
                    "special_tokens_mask": "special_tokens_mask",
                    "offsets": "offsets",
                }, // the tokenizer output(the key) map to the value
                "data_type": "single", // single or pair
                "process_data": [
                    ['sentence', { "is_pretokenized": false}],
                ],
                /*"data_type": "pair", // single or pair*/
                /*"process_data": [*/
                    /*['sentence_a', { "is_pretokenized": false}], */
                    /*['sentence_b', {}], the config of the second data must as same as the first*/
                /*],*/
            },
        },
        {
            "_name": "token_gather"
            "_status": ["train"],
            "config": {
                "tokens": "origin", // string or list, gather all filed
                "deliver": "all_tokens",
                "sort_method": "frequency", // sort method default is sort by tokens frequency( the first token is the most token )
                "data_set": {
                    "train": ['train', 'dev']
                },
            }
        }, //1
        {
            "_name": "token_gather",
            "_status": ["train"],
            "config":{
                "tokens": "label",
                "deliver": "label_id_map",
                "deliver_method": "create_map", // create_map means return label->id map, default "token_list" means return the tokens list
                "data_set": {
                    "train": ['train', 'dev']
                },
            }
        }, //2
        {
            "_name": "token_to_id",
            "_status": ["train", "predict", "online"],
            "config": {
                "data_pair": {
                    "label": "label_id"
                },
                "data_set": {                   // for different status, this processor will process different part of data
                    "train": ['train', 'dev'],
                    "predict": ['predict'],
                    "online": ['online']
                },
            }
        }, //3
        {
            "_name": "get_static_embedding",
            "_status": ["train"],
            "config": {
                "all_tokens": $ref,
                "restructure_embedding_id": {
                    "deliver": "token_id_map",
                },
                "deliver": "embedding",
                "deliver_method": "new"
            }
        }, //4

        {
            "_name": "token_to_id",
            "_status": ["train", "predict", "online"],
            "config": {
                "token_ids": $ref,
                "token": "origin_tokens",
                "ids": "origin_token_ids",
                "data_set": ['train', 'dev'],
            }
        }, //5
    ],
    "save": {
        "train": { // status, use "train&predict" means "train" & "predict"
            "data.train": "./train.pkl",
            "data.dev": "./dev.pkl",
            "token_ids": "./token_ids.pkl",
            "embedding": "./embedding.pkl",
            "label_ids": "./label_ids.pkl",
        },
        "predict": {
            "data&predict": "./predict.pkl"
        }
    },
    "_link": {
        "load.0.config.save_tokens":"process.2.config.tokens",
        "process.2.config.token_ids":"process.-1.config.token_ids",
    }
}
